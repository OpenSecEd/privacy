\title{Differential Privacy}
\subtitle{A short introduction}
\author[D. Bosk <dbosk@kth.se>]{Daniel Bosk}
\institute[KTH CSC]{%
  School of Computer Science and Communication\\
  KTH Royal Institute of Technology\\
  \email{dbosk@kth.se}
}
\date{12th February 2016}

\maketitle

\mode*

\begin{abstract}
  \input{abstract.tex}
\end{abstract}


\section{Differential Privacy}

\subsection{Background}

\begin{frame}
  \begin{block}{Dalenius, 1977}
    Nothing about an individual should be learnable from the database that 
    cannot be learned without access to the database.
  \end{block}

  \pause{}

  \begin{block}{Dwork, 2006\footfullcite{DifferentialPrivacy}}
    \begin{itemize}
      \item Dwork proves this is impossible.
      \item Actually you can learn things about individuals not in the 
        database.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}
  \begin{block}{Origin: Statistical databases}
    \begin{itemize}
      \item Privacy-preserving data analysis
      \item Statistical disclosure control
      \item Inference control
      \item Privacy-preserving data-mining
      \item Private data analysis
    \end{itemize}
  \end{block}

  \pause{}

  \begin{block}{Purpose}
    Protect \emph{any} one entry when publishing statistics about a database.
  \end{block}
\end{frame}

\begin{frame}
  \begin{block}{Non-interactive}
    \begin{itemize}
      \item Compute and publish some statistics.
      \item Database not used further.
      \item What can we infer from the released statistics?
    \end{itemize}
  \end{block}

  \pause{}

  \begin{block}{Interactive}
    \begin{itemize}
      \item Interactively answer statistical queries about the database.
      \item Queries and/or answers may be modified by the privacy mechanism.
    \end{itemize}
  \end{block}
\end{frame}

\subsection{Definition}

\begin{frame}
  \begin{block}{The idea}
    \begin{itemize}
      \item We want to protect \emph{any} one entry in this database.

        \pause{}

      \item Thus we want the result to be similar with and without this entry.

        \pause{}

      \item Thus we add random noise to achieve this.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}
  \begin{definition}[Differential 
    privacy\footfullcite{DifferentialPrivacyBook}]
    \begin{itemize}
      \item Let \(\M\) be a randomized function.
      \item Let \(D, D'\in \ker(\M)\) be databases differing on at most one 
        element.
      \item Let \(S\subseteq \im(\M)\).
      \item \(\M\) is \(\epsilon\)-differentially private if and only if
        \[\Pr[\M[D]\in S] \leq e^\epsilon \Pr[\M[D']\in S]\]
      \item \color{red} Probability taken over coin-tosses of \(\M\).
      \end{itemize}
  \end{definition}
\end{frame}

\begin{frame}
  \begin{remark}
    \begin{itemize}
      \item \enquote{Probability taken over coin-tosses of \(\M\).}
      \item As opposed to the \enquote{probability over the differences of \(D, 
            D'\)}.
    \end{itemize}
  \end{remark}
\end{frame}

\begin{frame}
  \begin{block}{Note}
    This protects a row even if the adversary knows every other row!
  \end{block}
\end{frame}

\begin{frame}
  \begin{definition}[Sensitivity]
    \begin{itemize}
      \item Let \(f\) be the true function, i.e.\ not \(\M\).
      \item \(D, D'\) as before.
      \item The sensitivity of \(f\) is \[
          \Delta f = \max_{D, D'} || f(D) - f(D') ||_{1}.
        \]
    \end{itemize}
  \end{definition}
\end{frame}

\begin{frame}
  \begin{theorem}
    \begin{itemize}
      \item \(f\) as before.
      \item \(\M_f\) is the mechanism adding noise to \(f\).

        \pause

      \item If \(\M_f\) adds noise with \(\Lap(\Delta f/\epsilon)\),
        then \(\M\) provides \(\epsilon\)-differential privacy.
    \end{itemize}
  \end{theorem}

  \pause

  \begin{remark}
    The needed noise depends only on the sensitivity of \(f\) and \(\epsilon\).
  \end{remark}
\end{frame}

\begin{frame}
  \begin{example}[Histograms]
    \begin{itemize}
      \item Histogram query with \(k\) cells.

        \pause{}

      \item Viewed as \(k\) counting queries.

        \pause{}

      \item Adding or removing a database entry can affect at most one of \(k\)
        cells.

        \pause{}

      \item Thus the histogram function has sensitivity \(1\).
    \end{itemize}
  \end{example}
\end{frame}


\section{Uses}

\subsection{Matching profiles}

\begin{frame}
  \begin{block}{The idea}
    \begin{itemize}
      \item Differential privacy is designed for statistics.
      \item {\color{red} We must be able to add noise.}

        \pause

      \item Computing a similarity score can be based on statistics:
        \begin{itemize}
          \item Is item \(X\) in the database? (\(Count(X)\))
          \item That's a statistical question.
        \end{itemize}
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}
  \begin{example}[Standard use]
    \begin{itemize}
      \item Have a statistical database.
      \item Each user's data corresponds to one row.

        \pause{}

      \item We protect individual users, by protecting the rows.
    \end{itemize}
  \end{example}

  \pause

  \begin{example}[BLIP\footfullcite{BLIP-2}]
    \begin{itemize}
      \item Let a user profile be the database.
        \begin{itemize}
          \item Each row is a \enquote{like}.
          \item Or a photo or a comment.
        \end{itemize}

        \pause{}

      \item Protect the individual rows.
        \begin{itemize}
          \item Protect the contents of the profile.
        \end{itemize}
    \end{itemize}
  \end{example}
\end{frame}

\begin{frame}
  \begin{block}{BLoom-and-flIP, BLIP}
    \begin{itemize}
      \item Hash the entries of the profile into a Bloom filter (\(L\)-bit 
        string).

        \pause

      \item Generate a random \(L\)-bit string.
      \item XOR them together --- adds noise.
    \end{itemize}
  \end{block}

  \pause

  \begin{block}{Results\footfullcite{BLIP-2}}
    \begin{itemize}
      \item BLIP is \(\epsilon\)-differentially private.
      \item So you can do reconstruction \enquote{within \(\epsilon\)}.
    \end{itemize}
  \end{block}
\end{frame}


\begin{frame}
  \begin{question}[Isn't it a guarantee?]
    \begin{itemize}
      \item You are guaranteed \(\epsilon\)-differential privacy.
      \item So there is something the attacker may learn.
      \item But how much is that?
      \item How do we choose \(\epsilon\)?
    \end{itemize}
  \end{question}
\end{frame}

\begin{frame}
  \begin{block}{Practical attacks\footfullcite{ChallengingDiffPriv}}
    \begin{itemize}
      \item \citet{ChallengingDiffPriv} constructs two practical attacks.
      \item These reconstruct the profiles in BLIP (within \(\epsilon\)).

        \pause

      \item This is one step closer to an informed choice for \(\epsilon\).
    \end{itemize}
  \end{block}
\end{frame}

\subsection{Analytics: collecting user data}

\begin{frame}
  \begin{example}[RAPPOR\footfullcite{RAPPOR}]
    \begin{itemize}
      \item Local differential privacy: yields correct population statistics.
      \item Estimate client-side distribution of strings.
      \item Used in Chrome to track distribution of configurations.
    \end{itemize}
  \end{example}
\end{frame}

\subsection{Machine learning}

\begin{frame}
  \begin{remark}
    \begin{itemize}
      \item Machine learning is \enquote{just statistics}.
    \end{itemize}
  \end{remark}

  \pause

  \begin{example}
    \begin{itemize}
      \item Can ensure that a model is differentially private.
      \item Differential privacy guarantees protection from over-fitting.
    \end{itemize}
  \end{example}
\end{frame}

\begin{frame}
  \begin{example}[Federated learning]
    \begin{itemize}
      \item Want a model trained on databases \(D_1, \dotsc, D_n\).
      \item The owner of \(D_i\) don't want to leak its database.
      \item Can create output \(O_i\) which yields differential privacy for 
        \(D_i\).
      \item E.g.~GBoard
    \end{itemize}
  \end{example}
\end{frame}


%%% REFERENCES %%%

\begin{frame}[allowframebreaks]
  \printbibliography
\end{frame}
